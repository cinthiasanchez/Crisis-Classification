{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "southeast-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configuration\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel \n",
    "from src import settings, load_embeddings, classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from os.path import basename\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "tqdm.pandas()\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d729881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((67001, 66), (13446, 66))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subset of experimental data\n",
    "data = pd.read_csv('../data/selected/Subset_Data_Uniques_LF.csv')\n",
    "\n",
    "#tweets from the 'negative class' = Not related to crisis\n",
    "negative_tweets = pd.read_csv('../data/selected/Subset_Not_related_tweets.csv')\n",
    "\n",
    "data.shape, negative_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e70e3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging both datasets, where tweets from the negative subset are \n",
    "# subsampled to balance our test data in each experiment.  \n",
    "data = pd.concat([data, negative_tweets], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "genetic-enzyme",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data, columns, features, model, experiments, runs=5, mkw={'n_jobs': 3}):\n",
    "    for i in range(1, runs + 1):  \n",
    "        restuls = pd.DataFrame([\n",
    "            classification.train_v2(data, columns, scenario, experiment, setting,\n",
    "                                    model(**mkw), features, True)\n",
    "            for scenario, experiments_ in experiments.items()\n",
    "                for experiment, setting in tqdm(experiments_.items(), scenario)\n",
    "        ])\n",
    "        mask = ~((restuls.test_1 < 50) | (restuls.test_0 < 50)) #experiments with at least 50 instances per class\n",
    "        restuls[mask].to_csv(f'../results/balanced/RF_{features}_{i}.csv', index=False)\n",
    "        \n",
    "def load_data(data, functor, col_index, params):\n",
    "    if functor is not None:\n",
    "        subset = pd.concat([data, functor(data, params)], axis=1, sort=False)\n",
    "        return subset, subset.columns[-col_index:]\n",
    "    return data.copy(), data.columns[-col_index:]\n",
    "\n",
    "def evaluate_embeddings(data, functor, col_index, feature_name, \n",
    "                        clf, experiments, add_lf=False, params=None):\n",
    "    subset, columns = load_data(data, functor, col_index, params)\n",
    "    columns = (list(columns) + list(data.columns[-49:-1])\n",
    "               if add_lf else columns)\n",
    "    evaluate(subset, columns, feature_name, clf, experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abfa4650",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf, experiments = RandomForestClassifier, settings.experimental_design()\n",
    "\n",
    "evaluate_embeddings(data, None, 48, 'LF', clf, experiments)\n",
    "evaluate_embeddings(data, load_embeddings.glove_features, 100, 'MT+GloVe', clf, experiments)\n",
    "evaluate_embeddings(data, load_embeddings.muse_features, 300, 'MUSE', clf, experiments)\n",
    "evaluate_embeddings(data, load_embeddings.muse_features, 300, 'MUSE+LF', clf, experiments, True)\n",
    "\n",
    "XLMR_params = {'ml':'xlm-roberta-base', 'model':AutoModel, 'tokenizer':AutoTokenizer, 'col_text':'fixed_text_expanded'}\n",
    "evaluate_embeddings(data, load_embeddings.model_features, 768, 'XLM-R', clf, experiments, False, XLMR_params) \n",
    "\n",
    "XLMT_params = {'ml':'cardiffnlp/twitter-xlm-roberta-base', 'model':AutoModel, 'tokenizer':AutoTokenizer, 'col_text':'fixed_text_expanded'}\n",
    "evaluate_embeddings(data, load_embeddings.model_features, 768, 'XLM-T', clf, experiments, False, XLMT_params) \n",
    "\n",
    "BERT_params = {'ml':'bert-base-uncased', 'model':AutoModel, 'tokenizer':AutoTokenizer, 'col_text':'translated'}\n",
    "evaluate_embeddings(data, load_embeddings.model_features, 768, 'MT+BERT', clf, experiments, False, BERT_params) \n",
    "\n",
    "BERTM_params = {'ml':'bert-base-multilingual-cased', 'model':AutoModel, 'tokenizer':AutoTokenizer, 'col_text':'fixed_text_expanded'}\n",
    "evaluate_embeddings(data, load_embeddings.model_features, 768, 'mBERT', clf, experiments, False, BERTM_params) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
